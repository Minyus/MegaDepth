{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "demo_standalone.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "z0-bJaKZeZLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "outputId": "ab0ff07a-b723-4f75-9f7b-aba5f0ca8b0a"
      },
      "cell_type": "code",
      "source": [
        "\"\"\" To use Google Drive with Colab, \n",
        "1. set use_google_drive to True, and\n",
        "2. specify a directory in Google Drive (Modify as in your Google Drive)\n",
        "(You will need to authorize manually.)\n",
        "\"\"\"\n",
        "use_google_drive = True\n",
        "workdir = '/content/drive/My Drive/Colab/MegaDepth/'\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "    if use_google_drive:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        # Create target directory & all intermediate directories if don't exists\n",
        "        if not os.path.exists(workdir):\n",
        "            os.makedirs(workdir)\n",
        "            print('## Directory: ' , workdir ,  ' was created.') \n",
        "        os.chdir(workdir)\n",
        "        print('## Current working directory: ', os.getcwd())\n",
        "except:\n",
        "    print('Run the code without using Google Drive.')\n",
        "        \n",
        "try:    \n",
        "    print('## Check the uptime. (Google Colab reboots every 12 hours)')\n",
        "    !cat /proc/uptime | awk '{print \"Uptime is \" $1 /60 /60 \" hours (\" $1 \" sec)\"}'\n",
        "    print('## Check the GPU info')\n",
        "    !nvidia-smi\n",
        "    print('## Check the OS') \n",
        "    !cat /etc/issue\n",
        "    print('## Check the Python version') \n",
        "    !python --version\n",
        "    print('## Check the memory')\n",
        "    !free -h\n",
        "    print('## Check the disk')\n",
        "    !df -h\n",
        "except:\n",
        "    print('Run the code assuming the environment is not Google Colab.')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "## Current working directory:  /content/drive/My Drive/Colab/MegaDepth\n",
            "## Check the uptime. (Google Colab reboots every 12 hours)\n",
            "Uptime is 4.03395 hours (14522.23 sec)\n",
            "## Check the GPU info\n",
            "Sun Apr 21 10:03:57 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.56       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "## Check the OS\n",
            "Ubuntu 18.04.2 LTS \\n \\l\n",
            "\n",
            "## Check the Python version\n",
            "Python 3.6.7\n",
            "## Check the memory\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            12G        444M        9.2G        916K        3.1G         12G\n",
            "Swap:            0B          0B          0B\n",
            "## Check the disk\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         359G   23G  318G   7% /\n",
            "tmpfs           6.4G     0  6.4G   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "tmpfs           6.4G   12K  6.4G   1% /var/colab\n",
            "/dev/sda1       365G   28G  338G   8% /opt/bin\n",
            "shm             6.0G     0  6.0G   0% /dev/shm\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "drive           200G  125G   76G  63% /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9xfL87_FVgTk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" .base_model \"\"\"\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "class BaseModel():\n",
        "    def name(self):\n",
        "        return 'BaseModel'\n",
        "\n",
        "    def initialize(self, opt):\n",
        "        self.opt = opt\n",
        "        self.gpu_ids = opt.gpu_ids\n",
        "        self.isTrain = opt.isTrain\n",
        "        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n",
        "        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "\n",
        "    def set_input(self, input):\n",
        "        self.input = input\n",
        "\n",
        "    def forward(self):\n",
        "        pass\n",
        "\n",
        "    # used in test time, no backprop\n",
        "    def test(self):\n",
        "        pass\n",
        "\n",
        "    def get_image_paths(self):\n",
        "        pass\n",
        "\n",
        "    def optimize_parameters(self):\n",
        "        pass\n",
        "\n",
        "    def get_current_visuals(self):\n",
        "        return self.input\n",
        "\n",
        "    def get_current_errors(self):\n",
        "        return {}\n",
        "\n",
        "    def save(self, label):\n",
        "        pass\n",
        "\n",
        "    # helper saving function that can be used by subclasses\n",
        "    def save_network(self, network, network_label, epoch_label, gpu_ids):\n",
        "        save_filename = '_%s_net_%s.pth' % (epoch_label, network_label)\n",
        "        save_path = os.path.join(self.save_dir, save_filename)\n",
        "        torch.save(network.cpu().state_dict(), save_path)\n",
        "        if len(gpu_ids) and torch.cuda.is_available():\n",
        "            network.cuda(device_id=gpu_ids[0])\n",
        "\n",
        "    # helper loading function that can be used by subclasses\n",
        "    def load_network(self, network, network_label, epoch_label):\n",
        "        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n",
        "        save_path = os.path.join(self.save_dir, save_filename)\n",
        "        print(save_path)\n",
        "        model = torch.load(save_path)\n",
        "        return model\n",
        "        # network.load_state_dict(torch.load(save_path))\n",
        "\n",
        "    def update_learning_rate():\n",
        "        pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EPmrGbaZVqZg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" pytorch_DIW_scratch \"\"\"\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from functools import reduce\n",
        "\n",
        "class LambdaBase(nn.Sequential):\n",
        "    def __init__(self, fn, *args):\n",
        "        super(LambdaBase, self).__init__(*args)\n",
        "        self.lambda_func = fn\n",
        "\n",
        "    def forward_prepare(self, input):\n",
        "        output = []\n",
        "        for module in self._modules.values():\n",
        "            output.append(module(input))\n",
        "        return output if output else input\n",
        "\n",
        "class Lambda(LambdaBase):\n",
        "    def forward(self, input):\n",
        "        return self.lambda_func(self.forward_prepare(input))\n",
        "\n",
        "class LambdaMap(LambdaBase):\n",
        "    def forward(self, input):\n",
        "        return list(map(self.lambda_func,self.forward_prepare(input)))\n",
        "\n",
        "class LambdaReduce(LambdaBase):\n",
        "    def forward(self, input):\n",
        "        return reduce(self.lambda_func,self.forward_prepare(input))\n",
        "\n",
        "\n",
        "pytorch_DIW_scratch = nn.Sequential( # Sequential,\n",
        "\tnn.Conv2d(3,128,(7, 7),(1, 1),(3, 3)),\n",
        "\tnn.BatchNorm2d(128),\n",
        "\tnn.ReLU(),\n",
        "\tnn.Sequential( # Sequential,\n",
        "\t\tLambdaMap(lambda x: x, # ConcatTable,\n",
        "\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\tnn.MaxPool2d((2, 2),(2, 2)),\n",
        "\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,32,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,32,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,32,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t),\n",
        "\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,32,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,32,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,32,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t),\n",
        "\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n",
        "\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\tnn.MaxPool2d((2, 2),(2, 2)),\n",
        "\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n",
        "\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(64,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(64,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(64,64,(11, 11),(1, 1),(5, 5)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\tnn.AvgPool2d((2, 2),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\tLambdaMap(lambda x: x, # ConcatTable,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tnn.AvgPool2d((2, 2),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\tnn.UpsamplingNearest2d(scale_factor=2),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n",
        "\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(64,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(64,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.Conv2d(64,64,(11, 11),(1, 1),(5, 5)),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t\t\tnn.UpsamplingNearest2d(scale_factor=2),\n",
        "\t\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,64,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(256,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\tnn.UpsamplingNearest2d(scale_factor=2),\n",
        "\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(32,32,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(64,32,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(64,32,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t\tnn.Conv2d(64,32,(11, 11),(1, 1),(5, 5)),\n",
        "\t\t\t\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t\t),\n",
        "\t\t\t\t\t\t),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n",
        "\t\t\t\t),\n",
        "\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(64,32,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(64,32,(5, 5),(1, 1),(2, 2)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(64,32,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t),\n",
        "\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,16,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,16,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,16,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,32,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(32,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(32,16,(11, 11),(1, 1),(5, 5)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t),\n",
        "\t\t\t\tnn.UpsamplingNearest2d(scale_factor=2),\n",
        "\t\t\t),\n",
        "\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\tLambdaReduce(lambda x,y,dim=1: torch.cat((x,y),dim), # Concat,\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,16,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(64,16,(3, 3),(1, 1),(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(64,16,(7, 7),(1, 1),(3, 3)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t\tnn.Sequential( # Sequential,\n",
        "\t\t\t\t\t\tnn.Conv2d(128,64,(1, 1)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(64,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\tnn.Conv2d(64,16,(11, 11),(1, 1),(5, 5)),\n",
        "\t\t\t\t\t\tnn.BatchNorm2d(16,1e-05,0.1,False),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t),\n",
        "\t\t\t\t),\n",
        "\t\t\t),\n",
        "\t\t),\n",
        "\t\tLambdaReduce(lambda x,y: x+y), # CAddTable,\n",
        "\t),\n",
        "\tnn.Conv2d(64,1,(3, 3),(1, 1),(1, 1)),\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5oAHh35uWUye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" .HG_model \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from torch.autograd import Variable\n",
        "# from .base_model import BaseModel\n",
        "import sys\n",
        "# import pytorch_DIW_scratch\n",
        "\n",
        "class HGModel(BaseModel):\n",
        "    def name(self):\n",
        "        return 'HGModel'\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        BaseModel.initialize(self, opt)\n",
        "\n",
        "        print(\"===========================================LOADING Hourglass NETWORK====================================================\")\n",
        "#         model = pytorch_DIW_scratch.pytorch_DIW_scratch\n",
        "        model = pytorch_DIW_scratch\n",
        "        model= torch.nn.parallel.DataParallel(model, device_ids = [0]) # model= torch.nn.parallel.DataParallel(model, device_ids = [0,1])\n",
        "        model_parameters = self.load_network(model, 'G', 'best_generalization') # model_parameters = self.load_network(model, 'G', 'best_vanila')\n",
        "        model.load_state_dict(model_parameters)\n",
        "        self.netG = model.cuda()\n",
        "\n",
        "\n",
        "    def batch_classify(self, z_A_arr, z_B_arr, ground_truth ):\n",
        "        threashold = 1.1\n",
        "        depth_ratio = torch.div(z_A_arr, z_B_arr)\n",
        "\n",
        "        depth_ratio = depth_ratio.cpu()\n",
        "\n",
        "        estimated_labels = torch.zeros(depth_ratio.size(0))\n",
        "\n",
        "        estimated_labels[depth_ratio > (threashold)] = 1\n",
        "        estimated_labels[depth_ratio < (1/threashold)] = -1\n",
        "\n",
        "        diff = estimated_labels - ground_truth\n",
        "        diff[diff != 0] = 1\n",
        "\n",
        "        # error \n",
        "        inequal_error_count = diff[ground_truth != 0]\n",
        "        inequal_error_count =  torch.sum(inequal_error_count)\n",
        "\n",
        "        error_count = torch.sum(diff) #diff[diff !=0]\n",
        "        # error_count = error_count.size(0)\n",
        "\n",
        "        equal_error_count = error_count - inequal_error_count\n",
        "\n",
        "\n",
        "        # total \n",
        "        total_count = depth_ratio.size(0)\n",
        "        ground_truth[ground_truth !=0 ] = 1\n",
        "\n",
        "        inequal_count_total = torch.sum(ground_truth)\n",
        "        equal_total_count = total_count - inequal_count_total\n",
        "\n",
        "\n",
        "        error_list = [equal_error_count, inequal_error_count, error_count]\n",
        "        count_list = [equal_total_count, inequal_count_total, total_count]\n",
        "\n",
        "        return error_list, count_list \n",
        "\n",
        "\n",
        "    def computeSDR(self, prediction_d, targets):\n",
        "        #  for each image \n",
        "        total_error = [0,0,0]\n",
        "        total_samples = [0,0,0]\n",
        "\n",
        "        for i in range(0, prediction_d.size(0)):\n",
        "\n",
        "            if targets['has_SfM_feature'][i] == False:\n",
        "                continue\n",
        "            \n",
        "            x_A_arr = targets[\"sdr_xA\"][i].squeeze(0)\n",
        "            x_B_arr = targets[\"sdr_xB\"][i].squeeze(0)\n",
        "            y_A_arr = targets[\"sdr_yA\"][i].squeeze(0)\n",
        "            y_B_arr = targets[\"sdr_yB\"][i].squeeze(0)\n",
        "\n",
        "            predict_depth = torch.exp(prediction_d[i,:,:])\n",
        "            predict_depth = predict_depth.squeeze(0)\n",
        "            ground_truth = targets[\"sdr_gt\"][i]\n",
        "\n",
        "            # print(x_A_arr.size())\n",
        "            # print(y_A_arr.size())\n",
        "\n",
        "            z_A_arr = torch.gather( torch.index_select(predict_depth, 1 ,x_A_arr.cuda()) , 0, y_A_arr.view(1, -1).cuda())# predict_depth:index(2, x_A_arr):gather(1, y_A_arr:view(1, -1))\n",
        "            z_B_arr = torch.gather( torch.index_select(predict_depth, 1 ,x_B_arr.cuda()) , 0, y_B_arr.view(1, -1).cuda())\n",
        "\n",
        "            z_A_arr = z_A_arr.squeeze(0)\n",
        "            z_B_arr = z_B_arr.squeeze(0)\n",
        "\n",
        "            error_list, count_list  = self.batch_classify(z_A_arr, z_B_arr,ground_truth)\n",
        "\n",
        "            for j in range(0,3):\n",
        "                total_error[j] += error_list[j]\n",
        "                total_samples[j] += count_list[j]\n",
        "\n",
        "        return  total_error, total_samples\n",
        "\n",
        "\n",
        "    def evaluate_SDR(self, input_, targets):\n",
        "        input_images = Variable(input_.cuda() )\n",
        "        prediction_d = self.netG.forward(input_images) \n",
        "\n",
        "        total_error, total_samples = self.computeSDR(prediction_d.data, targets)\n",
        "\n",
        "        return total_error, total_samples\n",
        "\n",
        "    def rmse_Loss(self, log_prediction_d, mask, log_gt):\n",
        "        N = torch.sum(mask)\n",
        "        log_d_diff = log_prediction_d - log_gt\n",
        "        log_d_diff = torch.mul(log_d_diff, mask)\n",
        "        s1 = torch.sum( torch.pow(log_d_diff,2) )/N \n",
        "\n",
        "        s2 = torch.pow(torch.sum(log_d_diff),2)/(N*N)  \n",
        "        data_loss = s1 - s2\n",
        "\n",
        "        data_loss = torch.sqrt(data_loss)\n",
        "\n",
        "        return data_loss\n",
        "\n",
        "    def evaluate_RMSE(self, input_images, prediction_d, targets):\n",
        "        count = 0            \n",
        "        total_loss = Variable(torch.cuda.FloatTensor(1))\n",
        "        total_loss[0] = 0\n",
        "        mask_0 = Variable(targets['mask_0'].cuda(), requires_grad = False)\n",
        "        d_gt_0 = torch.log(Variable(targets['gt_0'].cuda(), requires_grad = False))\n",
        "\n",
        "        for i in range(0, mask_0.size(0)):\n",
        " \n",
        "            total_loss +=  self.rmse_Loss(prediction_d[i,:,:], mask_0[i,:,:], d_gt_0[i,:,:])\n",
        "            count += 1\n",
        "\n",
        "        return total_loss.data[0], count\n",
        "\n",
        "\n",
        "    def evaluate_sc_inv(self, input_, targets):\n",
        "        input_images = Variable(input_.cuda() )\n",
        "        prediction_d = self.netG.forward(input_images) \n",
        "        rmse_loss , count= self.evaluate_RMSE(input_images, prediction_d, targets)\n",
        "\n",
        "        return rmse_loss, count\n",
        "\n",
        "\n",
        "    def switch_to_train(self):\n",
        "        self.netG.train()\n",
        "\n",
        "    def switch_to_eval(self):\n",
        "        self.netG.eval()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AmbZ7emqWRWv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" models.models \"\"\"\n",
        "\n",
        "\n",
        "def create_model(opt):\n",
        "    model = None\n",
        "#     from .HG_model import HGModel\n",
        "    model = HGModel(opt)\n",
        "    print(\"model [%s] was created\" % (model.name()))\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U4k_pieVWM7g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" data.data_loader \"\"\"\n",
        "\n",
        "def CreateDataLoader(_root, _list_dir, _input_height, _input_width, is_flip = True, shuffle =  True):\n",
        "    data_loader = None\n",
        "    from data.aligned_data_loader import AlignedDataLoader\n",
        "    data_loader = AlignedDataLoader(_root, _list_dir, _input_height, _input_width, is_flip, shuffle)\n",
        "    return data_loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRwD4ODFfbPf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "8d149142-4ddb-4b41-a745-093fb154d48e"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "# from options.train_options import TrainOptions\n",
        "# opt = TrainOptions().parse()  # set CUDA_VISIBLE_DEVICES before import torch\n",
        "# from options.test_options import TestOptions\n",
        "# opt = TestOptions().parse()  # set CUDA_VISIBLE_DEVICES before import torch\n",
        "import easydict\n",
        "opt = easydict.EasyDict({\n",
        "    'gpu_ids': '0',\n",
        "    'isTrain': False,\n",
        "    'checkpoints_dir': './checkpoints/',\n",
        "    'name': 'test_local'\n",
        "})\n",
        "\n",
        "# from data.data_loader import CreateDataLoader\n",
        "# from models.models import create_model\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "\n",
        "img_path = 'demo.jpg'\n",
        "\n",
        "model = create_model(opt)\n",
        "\n",
        "input_height = 384\n",
        "input_width  = 512\n",
        "\n",
        "\n",
        "def test_simple(model):\n",
        "    total_loss =0 \n",
        "    toal_count = 0\n",
        "    print(\"============================= TEST ============================\")\n",
        "    model.switch_to_eval()\n",
        "\n",
        "    img = np.float32(io.imread(img_path))/255.0\n",
        "    img = resize(img, (input_height, input_width), order = 1)\n",
        "    input_img =  torch.from_numpy( np.transpose(img, (2,0,1)) ).contiguous().float()\n",
        "    input_img = input_img.unsqueeze(0)\n",
        "\n",
        "    input_images = Variable(input_img.cuda() )\n",
        "    pred_log_depth = model.netG.forward(input_images) \n",
        "    pred_log_depth = torch.squeeze(pred_log_depth)\n",
        "\n",
        "    pred_depth = torch.exp(pred_log_depth)\n",
        "\n",
        "    # visualize prediction using inverse depth, so that we don't need sky segmentation (if you want to use RGB map for visualization, \\\n",
        "    # you have to run semantic segmentation to mask the sky first since the depth of sky is random from CNN)\n",
        "    pred_inv_depth = 1/pred_depth\n",
        "    pred_inv_depth = pred_inv_depth.data.cpu().numpy()\n",
        "    # you might also use percentile for better visualization\n",
        "    pred_inv_depth = pred_inv_depth/np.amax(pred_inv_depth)\n",
        "\n",
        "    io.imsave('demo.png', pred_inv_depth)\n",
        "    # print(pred_inv_depth.shape)\n",
        "#     sys.exit()\n",
        "\n",
        "\n",
        "\n",
        "test_simple(model)\n",
        "print(\"We are done\")\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========================================LOADING Hourglass NETWORK====================================================\n",
            "./checkpoints/test_local/best_generalization_net_G.pth\n",
            "model [HGModel] was created\n",
            "============================= TEST ============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
            "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
            "/usr/local/lib/python3.6/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
            "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingNearest2d is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "We are done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:141: UserWarning: Possible precision loss when converting from float32 to uint16\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}